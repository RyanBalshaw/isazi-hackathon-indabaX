{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s-VWbKVe6_z"
   },
   "source": [
    "\n",
    "# Welcome to the IndabaX 2024 Guided Hackathon with Isazi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KebcfZIRe6_2"
   },
   "source": [
    "\n",
    "## 1. Introduction\n",
    "*********************************\n",
    "\n",
    "#### 1.1. Problem Domain\n",
    "\n",
    "In the fast-paced retail sector, understanding and predicting sales volumes is crucial for effective inventory management, pricing strategy, and promotional planning. Accurately forecasting sales not only optimizes operational efficiencies but also enhances customer satisfaction by ensuring product availability and competitive pricing.\n",
    "\n",
    "#### 1.2 Challenge Description\n",
    "\n",
    "Participants are tasked with developing a predictive model that forecasts the future sales volumes of various products based on historical sales data. The dataset provided includes daily sales figures, promotional activities, and pricing information. The primary objective is to predict the volume of product sales for upcoming dates, which is critical for managing supply chain and marketing strategies.\n",
    "\n",
    "#### 1.3. Objectives\n",
    "- **Feature Engineering**: Identify and harness the influence of promotional activities and pricing strategies on sales volumes.\n",
    "- **Model Development**: Build robust time series forecasting models that can accurately predict sales volumes.\n",
    "\n",
    "#### 1.4. Evaluation Criteria\n",
    "\n",
    "The evaluation of forecasting models in this challenge uses two primary metrics: Overall Forecast Accuracy and Relative Bias. These metrics are calculated as follows:\n",
    "\n",
    "1. **Total Sales Volume**: Sum the actual sales volumes across all time series to obtain the total actual volume.\n",
    "2. **Total Predicted Volume**: Sum the predicted sales volumes across all time series.\n",
    "3. **Total Error**: Compute the absolute error between predicted and actual sales volumes for each prediction, and then sum these errors across all time series.\n",
    "4. **Relative Error**: Divide the total error by the total actual volume to obtain the relative error.\n",
    "5. **Forecast Accuracy**: Calculate forecast accuracy as `1 - Relative Error`.\n",
    "6. **Relative Bias**: Compute the relative bias by subtracting the total actual volume from the total predicted volume and dividing the result by the total actual volume. This metric indicates the tendency of the models to overestimate or underestimate the sales volumes.\n",
    "\n",
    "These metrics ensure a comprehensive evaluation of model performance:\n",
    "- **Forecast Accuracy** emphasizes the precision of individual predictions and is weighted towards time series with higher sales volumes, which are more significant for overall business performance.\n",
    "- **Relative Bias** measures the overall tendency of the predictions to be higher or lower than actual values, providing insight into the systemic accuracy of the models.\n",
    "\n",
    "Models will be judged not only on how accurately they predict sales volumes but also on how well they maintain balance, avoiding systematic overestimation or underestimation of sales.\n",
    "\n",
    "#### 1.5. Usefulness of the Challenge\n",
    "\n",
    "The solutions developed during this hackathon will help businesses:\n",
    "- **Enhance Decision Making**: Improve inventory and pricing decisions by predicting demand more accurately.\n",
    "- **Optimize Promotional Strategies**: Understand the impact of various promotional tactics on sales and adjust these strategies to maximize profitability.\n",
    "- **Reduce Waste and Shortages**: Better demand forecasts lead to more efficient supply chain management, reducing both excess stock and product shortages.\n",
    "\n",
    "#### 1.6. Outcome\n",
    "\n",
    "This challenge offers participants the opportunity to apply machine learning techniques to a real-world problem, enhancing their skills in data manipulation, model building, and evaluation. The top-performing models could potentially be implemented in real retail environments, demonstrating the practical value of predictive analytics in business contexts.\n",
    "\n",
    "#### 1.7. Support Channels\n",
    "* SwapCard Hackathon Discussion Forum\n",
    "\n",
    "#### 1.8. Mentor and Support\n",
    "* Kilian Massa\n",
    "\n",
    "#### 1.9. Submission Process\n",
    "The evaluation of forecasting models in this competition focuses on two key metrics:\n",
    "Overall Forecast Accuracy and Relative Bias.\n",
    "\n",
    "### Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TynKi6REe6_3"
   },
   "source": [
    "## 0. Setup\n",
    "****************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KgLNDWZe6_3"
   },
   "source": [
    "#### 0.1 Mounting Google Drive\n",
    "\n",
    "Running this Notebook on Google Colab?\n",
    "\n",
    "* If yes, run the next cell.\n",
    "* If No, Skip to 0.3: Update working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "pK1pvutZe6_4",
    "outputId": "1a55082f-8163-487d-f558-694ada52f6ef"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Vz9IASe6_4"
   },
   "source": [
    "#### 0.2. Update Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "b65IeHHCfBPf",
    "outputId": "1e18bf22-8d6d-483f-f935-caa4eb8e7ee0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEAnqjSoe6_5",
    "outputId": "cee5a060-bb69-407a-ed05-43477d89ff2a"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qJoulnAe6_5"
   },
   "source": [
    "#### 0.3. Install Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cC51bK5fVZuP",
    "outputId": "3be65c38-39e2-4391-f034-a534d8257c9a"
   },
   "outputs": [],
   "source": [
    "!pip install pandas \"gluonts[mxnet, torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gge3isoZe6_6",
    "outputId": "19429c0e-d8ed-4638-c162-8e8a4f548cfb"
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHv-ZQ-He6_6"
   },
   "source": [
    "#### 0.4 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qLpjdkYe6_6"
   },
   "outputs": [],
   "source": [
    "import json  # Standard library for working with JSON data\n",
    "\n",
    "import pandas as pd  # Powerful data manipulation and analysis library\n",
    "import numpy as np  # Library for numerical computations\n",
    "np.bool = np.bool_\n",
    "import matplotlib.pyplot as plt  # Library for creating static, animated, and interactive visualizations\n",
    "import seaborn as sns  # Data visualization library based on matplotlib, provides a high-level interface for drawing attractive and informative statistical graphics\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # Utility for splitting the dataset into training and testing sets\n",
    "from sklearn.preprocessing import StandardScaler  # Utility for standardizing features by removing the mean and scaling to unit variance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Metrics for evaluating the performance of a regression model\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset  # GluonTS utility for creating datasets from pandas dataframes\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions  # Utility for generating evaluation predictions from a trained model\n",
    "from gluonts.evaluation import Evaluator  # Utility for evaluating the performance of time series forecasting models\n",
    "\n",
    "from gluonts.mx import SimpleFeedForwardEstimator, Trainer # GluonTS classes for creating and training a simple feed-forward neural network model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWzbDcMqe6_6"
   },
   "source": [
    "#### 0.5 Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvkyvtyTe6_6"
   },
   "outputs": [],
   "source": [
    "# Dataset paths and column definitions\n",
    "time_series_id_col = 'product_code'\n",
    "date_col = 'sales_date'\n",
    "target_col = 'volume'\n",
    "freq = \"1D\"\n",
    "prediction_length = 28\n",
    "data_path = \"data/isazi_ts_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_X2XDRee6_6"
   },
   "source": [
    "\n",
    "## 2. Loading Data\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "282JNjxse6_6",
    "outputId": "40a926dd-9f12-4bea-a36b-f06ea9c12e1b"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "# Display the first few rows of the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OW2yEEMe6_7"
   },
   "source": [
    "## 3. Data Preprocessing\n",
    "********************************\n",
    "In this section, we'll preprocess the data to make it suitable for modeling. This includes handling missing values, encoding categorical variables, and scaling numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tb42S6Wje6_7"
   },
   "source": [
    "The dataset consists of the following columns:\n",
    "- `sales_date`: Date of the sales record.\n",
    "- `volume`: Number of units sold.\n",
    "- `rel_promo_price`: Relative promotional price.\n",
    "- `is_promo`: Indicator if the item was on promotion.\n",
    "- `is_single_price_promo`: Indicator if there was a single price promotion.\n",
    "- `is_multibuy_promo`: Indicator if there was a multibuy promotion.\n",
    "- `rsp`: Retail selling price.\n",
    "- `planned_promo_vol`: Planned promotional volume.\n",
    "- `product_code`: Unique product identifier.\n",
    "\"\"\"\n",
    "\n",
    "`NB`: all the promo covariates (is_promo, planned_promo_vol, rel_promo_price, is_multibuy_promo) are reported by the supplier, and the volume we are predicting is the volume sold by the retailer, i.e. the supplier is essentially giving us an estimate of the retailer's promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NN-o6P8Ce6_7",
    "outputId": "4b574a72-c3a7-4cc4-82bc-ceba65d233c6"
   },
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"Data types and missing values:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFLWdW0Ne6_7",
    "outputId": "b086811c-3929-4314-c37d-862d66e6fab3"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "v5JakYmFe6_7",
    "outputId": "05103039-2d51-4d90-b35d-0d0b9433e19e"
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuYCJm09e6_7",
    "outputId": "8d591def-86be-44bf-e888-59c3c3ad6451"
   },
   "outputs": [],
   "source": [
    "#get unique product\n",
    "print(\"Number of Unique Products:\")\n",
    "df['product_code'].unique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfMp46oLe6_7",
    "outputId": "47dcb939-9bf9-478a-9da4-10dd6f0be9d9"
   },
   "outputs": [],
   "source": [
    "# Convert 'sales_date' to datetime\n",
    "df['sales_date'] = pd.to_datetime(df['sales_date'])\n",
    "print(\"Dataset after data type conversions:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "OChmEyQHe6_7",
    "outputId": "d97685d1-840e-4b4f-a8bc-1fbd752ca73f"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"Dataset after removing duplicates:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "x5k5Fk9Te6_7",
    "outputId": "88e69aa9-c168-41ef-d793-907de3717ddc"
   },
   "outputs": [],
   "source": [
    "# Handle missing values (if any)\n",
    "df.ffill()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBJ6Lcm9e6_7"
   },
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "********************************\n",
    "\n",
    "Before building the model, it's essential to understand the data through exploratory data analysis (EDA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "spxcHHWge6_7",
    "outputId": "4d454b20-be6e-4c60-b05c-a2a721d4556c"
   },
   "outputs": [],
   "source": [
    "# Plot sales volume over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='sales_date', y='volume', data=df)\n",
    "plt.title('Sales Volume Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales Volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "Bx8g7lz5e6_8",
    "outputId": "586bf27a-dd37-4afd-f349-e6b8af6a42db"
   },
   "outputs": [],
   "source": [
    "# Distribution of sales volume\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df['volume'], bins=30, kde=True)\n",
    "plt.title('Distribution of Sales Volume')\n",
    "plt.xlabel('Sales Volume')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "GK-ECFOHe6_8",
    "outputId": "1700b391-9e0a-4f08-e984-9f3cf456e48e"
   },
   "outputs": [],
   "source": [
    "# Sales volume vs. promotion\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='is_promo', y='volume', data=df)\n",
    "plt.title('Sales Volume vs. Promotion')\n",
    "plt.xlabel('Promotion')\n",
    "plt.ylabel('Sales Volume')\n",
    "plt.show()\n",
    "\n",
    "# Average sales volume by promotion\n",
    "print(\"Average sales volume by promotion:\")\n",
    "print(df.groupby('is_promo')['volume'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "BCkp0tBYe6_8",
    "outputId": "627f3015-dbe1-4da3-b78c-29d67ffca797"
   },
   "outputs": [],
   "source": [
    "# Sales volume vs. price\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='rsp', y='volume', data=df)\n",
    "plt.title('Sales Volume vs. Price')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Sales Volume')\n",
    "plt.show()\n",
    "\n",
    "# Correlation between price and sales volume\n",
    "print(\"Correlation between price and sales volume:\")\n",
    "print(df[['rsp', 'volume']].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "V9vegGxse6_8",
    "outputId": "b565e5fb-67d8-4bb5-f3e0-6217a24c8d82"
   },
   "outputs": [],
   "source": [
    "# Sales volume by product\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='product_code', y='volume', data=df)\n",
    "plt.title('Sales Volume by Product')\n",
    "plt.xlabel('Product ID')\n",
    "plt.ylabel('Sales Volume')\n",
    "plt.show()\n",
    "\n",
    "# Average sales volume by product\n",
    "print(\"Average sales volume by product:\")\n",
    "print(df.groupby('product_code')['volume'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 837
    },
    "id": "YKIgLJVQe6_8",
    "outputId": "ba3b4143-8af1-4879-83b0-2b630f7aa0d3"
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmsPSYUMe6_8"
   },
   "source": [
    "## 5. Feature Engineering\n",
    "********************************\n",
    "Feature engineering involves creating new features or modifying existing ones to improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "lLewvgxPe6_8",
    "outputId": "7609fd83-84c3-40b4-c8a4-e99a512d08ae"
   },
   "outputs": [],
   "source": [
    "# Extract additional time features\n",
    "df['year'] = df['sales_date'].dt.year\n",
    "df['month'] = df['sales_date'].dt.month\n",
    "df['day'] = df['sales_date'].dt.day\n",
    "df['dayofweek'] = df['sales_date'].dt.dayofweek\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuAq59wZe6_8"
   },
   "outputs": [],
   "source": [
    "# Drop the original date column\n",
    "# df.drop('sales_date', axis=1, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZx6IwJ2e6_8"
   },
   "source": [
    "## 6. Data Preparation\n",
    "********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmX6q5CHe6_8"
   },
   "source": [
    "#### 6.1 Split into Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfD6TOW8e6_8"
   },
   "outputs": [],
   "source": [
    "def split_time_series(df, prediction_length):\n",
    "    \"\"\"\n",
    "    Function to split off a df of time series into time series, where the second time series\n",
    "    includes the last `prediction_length` time steps.\n",
    "    \"\"\"\n",
    "    # Create an empty dataframe for train and validation sets\n",
    "    train_df = pd.DataFrame()\n",
    "    validation_df = pd.DataFrame()\n",
    "\n",
    "    # Group by the time series identifier\n",
    "    grouped = df.groupby(time_series_id_col)\n",
    "\n",
    "    # Iterate over each group (i.e., each individual time series)\n",
    "    for item_id, group in grouped:\n",
    "        # Sort the group by date if it's not already sorted\n",
    "        group = group.sort_index()\n",
    "\n",
    "        # Define the split point\n",
    "        split_point = len(group) - prediction_length\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        train_group = group.iloc[:split_point]\n",
    "        validation_group = group\n",
    "\n",
    "        # Append to the respective dataframes\n",
    "        train_df = pd.concat([train_df, train_group])\n",
    "        validation_df = pd.concat([validation_df, validation_group])\n",
    "\n",
    "    return train_df, validation_df\n",
    "\n",
    "train_df, test_df = split_time_series(df, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "2j6zvEZje7AB",
    "outputId": "78cbe71d-628f-4250-f546-240273fff593"
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "6WWg78Owe7AB",
    "outputId": "2b33d345-a85d-4b59-fd54-0e28bb0fe0fd"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjNnlAKJe7AB"
   },
   "source": [
    "#### 6.2. Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "No5qno65e7AB",
    "outputId": "00367a83-ef91-4fe2-ed4e-539d4d23a490"
   },
   "outputs": [],
   "source": [
    "# Encode categorical variables (if any)\n",
    "# For simplicity, we'll use one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['product_code'], drop_first=True)\n",
    "df\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "df[['rel_promo_price', 'rsp', 'planned_promo_vol']] = scaler.fit_transform(df[['rel_promo_price', 'rsp', 'planned_promo_vol']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utR749rAe7AB"
   },
   "source": [
    "## 7. Model Development\n",
    "********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfNXtHlCe7AC"
   },
   "source": [
    "#### 7.1. Model Training\n",
    "We will train a simple feedforward neural network to predict future sales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaPSeD7Ge7AC",
    "outputId": "3b27e2b3-885a-4ce3-aa44-2ef0f0a08d8a"
   },
   "outputs": [],
   "source": [
    "def train_ffnn_predictor(df):\n",
    "    \"\"\"\n",
    "    A very basic feed forward predictor using GluonTS with a training and validation set.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.set_index(date_col, inplace=True) # GluonTS wants the timestamp to be the index\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    train_df, val_df = split_time_series(df, prediction_length)\n",
    "    # Create the Pandas datasets\n",
    "    train_ds = PandasDataset.from_long_dataframe(train_df,\n",
    "                                                target=target_col,\n",
    "                                                item_id=time_series_id_col,\n",
    "                                                freq=freq)\n",
    "\n",
    "    val_ds = PandasDataset.from_long_dataframe(val_df,\n",
    "                                                target=target_col,\n",
    "                                                item_id=time_series_id_col,\n",
    "                                                freq=freq)\n",
    "\n",
    "    # Train a feed forward estimator\n",
    "    estimator = SimpleFeedForwardEstimator(\n",
    "        num_hidden_dimensions=[10],\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=100,\n",
    "        trainer=Trainer(ctx=\"cpu\", epochs=10, learning_rate=1e-3, num_batches_per_epoch=100),\n",
    "    )\n",
    "    predictor = estimator.train(training_data=train_ds, validation_data=val_ds)\n",
    "    return predictor\n",
    "\n",
    "ffnn_predictor = train_ffnn_predictor(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClRLRlFCe7AC"
   },
   "source": [
    "#### 7.2. Model Prediction\n",
    "\n",
    "Using the trained model to make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YH918f19e7AC"
   },
   "outputs": [],
   "source": [
    "def make_predictions(predictor, df, feature_columns=[]):\n",
    "    \"\"\"\n",
    "    Make predictions with a GluonTS predictor and return them as a df.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.fillna(0) # Replace any NaNs with 0\n",
    "    df.set_index(date_col, inplace=True) # GluonTS wants the timestamp to be the index\n",
    "    dataset = PandasDataset.from_long_dataframe(df,\n",
    "                                                target=target_col,\n",
    "                                                item_id=time_series_id_col,\n",
    "                                                freq=freq,\n",
    "                                                feat_dynamic_real=feature_columns)\n",
    "\n",
    "    forecast_it, _ = make_evaluation_predictions(\n",
    "        dataset=dataset,\n",
    "        predictor=predictor,\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "\n",
    "    # Initialize a list to hold all the records before converting to a DataFrame\n",
    "    records = []\n",
    "    for forecast in forecasts:\n",
    "        item_id = forecast.item_id\n",
    "        start_timestamp = forecast.start_date.start_time\n",
    "\n",
    "        # GluonTS does probabilistic forecasting with a number of samples\n",
    "        # Calculate mean targets across all samples for each date position\n",
    "        mean_targets = forecast.samples.mean(axis=0)\n",
    "        for i, target in enumerate(mean_targets):\n",
    "            # Calculate what the timestamp should be for each predicted target\n",
    "            timestamp = start_timestamp + i * pd.to_timedelta(freq)\n",
    "            # Store the prediction as a record\n",
    "            records.append({date_col: timestamp, time_series_id_col: item_id, target_col: target})\n",
    "\n",
    "    # Convert the predictions from a list of records into a DataFrame\n",
    "    preds_df = pd.DataFrame(records)\n",
    "    preds_df.set_index(date_col, inplace=True)\n",
    "    return preds_df\n",
    "\n",
    "output_preds_path = \"my_predictions.csv\"\n",
    "preds_df = make_predictions(ffnn_predictor, test_df)\n",
    "preds_df.to_csv(output_preds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lednvroZe7AC"
   },
   "source": [
    "#### 7.3. Model Evaluation\n",
    "\n",
    "We compute the error between the forecasts and the actual sales to evaluate our model's performance. For this we main use overall relative error bias, the metric for the hackathon should probably be a weighted combination like `(1 - rE - 0.5*abs(rB)\n",
    ")*100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rkr6zfOIe7AC",
    "outputId": "6389a5ca-35b9-4b59-b174-db17963932cc"
   },
   "outputs": [],
   "source": [
    "def compute_error(actuals_path, preds_path):\n",
    "    \"\"\"\n",
    "    Computes the relative error and relative bias from csv files of predictions and actual targets.\n",
    "    \"\"\"\n",
    "    # Read predicted and actuals from their respective files if not already provided\n",
    "    actuals_df = pd.read_csv(actuals_path, parse_dates=[date_col])\n",
    "    preds_df = pd.read_csv(preds_path, parse_dates=[date_col])\n",
    "\n",
    "    # Rename 'target' column to add suffix for 'preds' and 'actuals' respectively\n",
    "    actuals_df.rename(columns={target_col: f'{target_col}_actuals'}, inplace=True)\n",
    "    preds_df.rename(columns={target_col: f'{target_col}_preds'}, inplace=True)\n",
    "\n",
    "    # Merge the two dataframes on the timestamp column and the time series identifier column\n",
    "    df = pd.merge(actuals_df, preds_df, on=[date_col, time_series_id_col])\n",
    "\n",
    "    actual_var = target_col + '_actuals'\n",
    "    pred_var = target_col +'_preds'\n",
    "    measure_level = [time_series_id_col, date_col]\n",
    "\n",
    "    # Drop all unecessary columns\n",
    "    keep_vars = list(set(measure_level + [actual_var, pred_var]))\n",
    "    df_filtered = df.dropna(subset=[actual_var])[keep_vars]\n",
    "    df_filtered.rename(columns={actual_var: 'A', pred_var: 'P'}, inplace=True)\n",
    "\n",
    "    # Group by measure_level and aggregate A and P\n",
    "    grouped = df_filtered.groupby(measure_level, observed=False).agg(A=('A', 'sum'), P=('P', 'sum'))\n",
    "\n",
    "    # Calculate the errors initially at measure_level (not the absolute sum yet)\n",
    "    grouped['E'] = (grouped['A'] - grouped['P']).abs()\n",
    "\n",
    "    # Aggregate all data to one row\n",
    "    grouped = grouped.sum()\n",
    "\n",
    "    # Calculate relative error (rE) and relative bias (rB)\n",
    "    grouped['rE'] = grouped['E'] / grouped['A']\n",
    "    grouped['rB'] = (grouped['P'] - grouped['A']) / grouped['A']\n",
    "    return grouped\n",
    "\n",
    "output_preds_path = \"my_predictions.csv\"\n",
    "report = compute_error(data_path, output_preds_path)\n",
    "print(\"Error Report:\")\n",
    "print(report)\n",
    "\n",
    "forecast_acc = (1 - (report.rE)) * 100\n",
    "forecast_bias = report.rB * 100\n",
    "acc_bias = forecast_acc - 0.5 * abs(forecast_bias)\n",
    "print(f\"Forecast Accuracy is: {forecast_acc:.2f}%\")\n",
    "print(f\"Forecast Bias is: {forecast_bias:.2f}%\")\n",
    "print(f\"Bias-weighted Accuracy: {acc_bias:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6FRRe8gRlx0"
   },
   "source": [
    "#### 7.4. DeepAR-based forecast\n",
    "\n",
    "The simple feed forward predictor used previously is unable to make use of any of our features for prediction, making its usefulness limited. GluonTS provides a host of different deep-learning forecasting models which you can try out to see which works best, you can read about each one [here](https://ts.gluon.ai/stable/getting_started/models.html).\n",
    "\n",
    "In this example, we use the DeepAR probabilistic forecasting model which you can read about [here](https://https://www.sciencedirect.com/science/article/pii/S0169207019301888?via%3Dihub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "86cd0c70bb594e21a2a77410af10c31a",
      "ecf0939fb576489e8646757732972011",
      "a36c51cbdbb049b6b363e427ef2d8e7d",
      "7d34d4e2e16b4cea80d6f38d8eb4d30c",
      "22872161d7bf4e0580a2f099c33c181d",
      "2ccbbb5de1a440cbb5439719d3cfb87b",
      "df438978cc234b53ab62f8cc81ced490",
      "f37cc7e1b84c45b6b5942e372a958440",
      "0631d038fc4f45df9b83d3c0945b224a",
      "a515fd81fc134017a3c6c91e4d660df5",
      "5471e20857a44b8d9cfff0e4ed5c1ddd",
      "283c74ad1f444c2587095473754bad14",
      "d7896933827341eea71df4237906c24b",
      "764469e472684de296edbad492dd441d",
      "3a3708afe45e4f1c9b7fc0dc55c5aee7",
      "1f148900016e4df98191eb2b11fd654f",
      "53781f7ebe494a78b1e2d133e3a9344b",
      "4e4b71efbb854d46a8b7f3354f9e66fe",
      "42ddfccf2fda48938b4093a45a5194af",
      "544b6d30fab54132b37b75b9deaffbcc",
      "7f0e7069274344ed8bfe5af2d7f81833",
      "72f5099521584bd89ae2f0e4b5b2920b",
      "34fd5837445f4849bef113dcc4e7362c",
      "292589f2c9c94bd7948f261d5b0f5cfa",
      "2487c6352d5b491fa83d33cc4e03bbb5",
      "41fc7c8a7dbf415c87ee33dcef07fc8f",
      "ec7c99fde05a42088d882a6e95e3c976",
      "c2153a89d6f24d1bb3f4795ddf63ab71",
      "5b0b6fbef85c44778b580b2c060fdda4",
      "44fb87418b164344b0990aaddb702b2d",
      "916865cff5ac4dbc8197148b35708a26",
      "bfb85cd8897d4f90811c05c5a2160ada",
      "5d0f489b027d45ca983e5cf479157476",
      "2cea9bd4ffa7496fb00dbef5e9837916",
      "1005454c769d40b2911f8aa60c8f521e",
      "87355b48ae09416aad29d706cd97ef07",
      "74101092f1ca42108ab1e14d8a03bba4",
      "9e792230145548b28ada741a876d4ff2",
      "80f69f70ecdf40fab2ce2acd96d92048",
      "c65744eb76b64f6fbd2ea639c340c973",
      "15f39ee00aab4b47ac269715539e0e80",
      "6c1405e71250414788acd63784a38e85",
      "cf475b2aba1c48e69cd58f912d3f2fec",
      "3101ce8918fa43a093d9e2b68bbb19b3",
      "b75ab6eaa469429090a092fe260f3eb7",
      "4d49c36dd4c544b2b032dd6a076d47d6",
      "1e8d1f394b1f4cf3ac06876789c5806a",
      "490ab4014c984750a85d62be1f446ea3",
      "92002898437c47f49a3f19f797f10bee",
      "6d981e4795ae48eda957f1072ad75da7",
      "eae76403f5ef486db35b60c1ed2473d9",
      "8214dd57ebd94afb835937210502bebe",
      "c2b55cc269fb4886b3fd0ce9d3552ec9",
      "b554cd33a5e44990a7cd296a93c1c453",
      "336314df840e47bd9ddbbb4dda202542",
      "1777ed3dafd5455881eb0ff0922c4082",
      "fd2116d28f0f42ecaae4b3204fc1697d",
      "9a4d328a36a7436eb359d72c34d1c1df",
      "fa77e0ed4b2b42968c6f62f4c8bdf712",
      "928e0d74560e4d47b8909a896f7dac10",
      "30c08200509f4fde9b432e9c57ba32c9",
      "699137710d5b40b2bf54935a30c5f159",
      "a3de1ed17abc4219b4796927c87fd0d6",
      "e0dc31bef6294453964e69bbee3942ba",
      "ac19867777ab4b08bfaffb9edf6db43c",
      "0ef322af7d96412cbb4cfe6654f61d25",
      "162e6c59d14c4935b26147b0f7b99245",
      "b4fba9bd19da423a8c0885d3709f6835",
      "e0fbaf7a90984f119b3fa17a6a759196",
      "3fe3aa6238c74512854dddb2d1fc0e2a",
      "90ae5513a60c47b6933af656c1fd3fbf",
      "3fb6ef48e1f2427aba626837f7d58ee1",
      "3d908f27a30e48a7a6bfd6963e6b2eb6",
      "a66c60ea413a4feeaf57fd5a53e40fd1",
      "4465d57c271443fd82f685735556c5bf",
      "83345df3b485468cbbaac38879ecfe53",
      "0950ff5feddf48b2877d2cdfc2a76cd1",
      "ab25991d6ff748d4a2d69e3bf82ce001",
      "24aa51c7f1ce4eaaa8ca77e3112efc60",
      "458c8e2633fe44fc91f6d2fdf04d84ff",
      "25ffa101c16b425c9617943f54a5a2b3",
      "da388c0acc0c428681f0008fecbcf365",
      "39e120a3a773408c9d0b10837668e0f3",
      "71056d8983d14072a2f289d143e66b79",
      "e77183998f8a42a8bcd1e4eb7e9f8dea",
      "b963adc5dfaf4f30addf5464ffca5944",
      "2c85b45b2ebf47038351902f17a8bb89",
      "8cf84ed1dce54e7dbc1c264e03a55962",
      "7dcd65ab4ce2421aa80a3f313c7ee89b",
      "029354a6c84445858c51adb5f7c04566",
      "a98674717f1047a99aa7bcb022d2df48",
      "294dd74d4ba24a66887686c76b3a1756",
      "616162e702724742995fa7b9341cc40a",
      "8b97a99c457b46ef9cd2905238dd7541",
      "37dec20088cf431584784f98f23949a7",
      "3a189c8e2cbf46d9a10f3a3ec322788f",
      "3db9eddf4ba94d859ef7dc77d27adb5f",
      "80c3ae278bf24a8883a20e7d7930d4f5",
      "2fccc18499724a30986597c389876b88",
      "c5d9444ae2bb44a58082607128367038",
      "4747ffe4981347b08753f26246736616",
      "98bd4741ef664903b6afe7b816879ede",
      "ae9b0257b4184a2aa43923960e1c923c",
      "73389ee04ea04b8cb88c9ce97ae64be2",
      "bf563d886c1d496aa30f3e11c08cb585",
      "b9671f80830740198bbbc959b86d9bfa",
      "1ddf6105389a4a629639cc74e9a3c623",
      "31dce970d103426c9e1798cbadecd9fa",
      "993638ff412e48019038201604086b2c",
      "8dd9370d3c9e4716a68b3692235a76ce",
      "232d139ed92348c49b50ffc96882a761",
      "a1e751166e2f4e77b0e0e19588198dff",
      "5ed406234eb347e29c641db2ee930e8b",
      "0630362c1ad64afa8a6a21794aac8e3f",
      "9f505bc8f1d04542af369f2025a90524",
      "ad3f959ac9344b9e95974beab0aa0c50",
      "e630a05fc2004e19ad74ff580eafc1b6",
      "4ed5005de641452194fb91c590553f77",
      "056dad4669cf4b13a26bdcf5ee521e38",
      "c69427953d2547e084f8188838c4a322",
      "07f72738852d4f539196c22d7c39d5ef",
      "85c06de4029d40a3a6bea06999a69dec",
      "adc78d62ee9f4f13a03935826e83dda1",
      "186a3a77accb450288939eec2baf9d97",
      "8efbf559ca7044aeb97ede5061fd3c87",
      "eed7d6826b354bfc9588ca7c137d8ede",
      "bdf27db9a80e449f97352444c54b8d38",
      "9d762f0ff5d14a70ac778450e9599428",
      "5bc5c8eb566b42d9a52081ed9719999d",
      "59940e11161840d5b4cc67b15551bdb3",
      "d59f272663dd4f1fade47556db7a6078",
      "a165973c6de3430b96af66d6bef4cbb5"
     ]
    },
    "id": "vSs9W1pyRlGP",
    "outputId": "3232191d-1076-4bb6-f6b1-ee76e47f8316"
   },
   "outputs": [],
   "source": [
    "from gluonts.torch import DeepAREstimator\n",
    "\n",
    "def train_deep_ar_predictor(df, feature_columns):\n",
    "    \"\"\"\n",
    "    A more advance model able to make use of features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.set_index(date_col, inplace=True) # GluonTS wants the timestamp to be the index\n",
    "    df = df.fillna(0) # Replace any NaNs with 0\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    train_df, val_df = split_time_series(df, prediction_length)\n",
    "    # Create the Pandas datasets\n",
    "    train_ds = PandasDataset.from_long_dataframe(train_df,\n",
    "                                                target=target_col,\n",
    "                                                item_id=time_series_id_col,\n",
    "                                                freq=freq,\n",
    "                                                feat_dynamic_real=feature_columns)\n",
    "\n",
    "    val_ds = PandasDataset.from_long_dataframe(val_df,\n",
    "                                                target=target_col,\n",
    "                                                item_id=time_series_id_col,\n",
    "                                                freq=freq,\n",
    "                                                feat_dynamic_real=feature_columns)\n",
    "\n",
    "    # Train a feed forward estimator\n",
    "    estimator = DeepAREstimator(\n",
    "        prediction_length=prediction_length,\n",
    "        freq=freq,\n",
    "        num_feat_dynamic_real=len(feature_columns),\n",
    "        trainer_kwargs={\"max_epochs\": 10}\n",
    "    )\n",
    "    predictor = estimator.train(training_data=train_ds, validation_data=val_ds)\n",
    "    return predictor\n",
    "\n",
    "# Define which features to use and train\n",
    "feature_columns=['is_promo', 'rel_promo_price', 'planned_promo_vol', 'rsp']\n",
    "deep_ar_predictor = train_deep_ar_predictor(train_df, feature_columns)\n",
    "\n",
    "# Make predictions\n",
    "output_preds_path = \"my_deep_ar_predictions.csv\"\n",
    "preds_df = make_predictions(deep_ar_predictor, test_df, feature_columns)\n",
    "preds_df.to_csv(output_preds_path)\n",
    "\n",
    "# Evaluate\n",
    "report = compute_error(data_path, output_preds_path)\n",
    "print(\"DeepAR Error Report:\")\n",
    "print(report)\n",
    "\n",
    "forecast_acc = (1 - (report.rE)) * 100\n",
    "forecast_bias = report.rB * 100\n",
    "acc_bias = forecast_acc - 0.5 * abs(forecast_bias)\n",
    "print(f\"Forecast Accuracy is: {forecast_acc:.2f}%\")\n",
    "print(f\"Forecast Bias is: {forecast_bias:.2f}%\")\n",
    "print(f\"Bias-weighted Accuracy: {acc_bias:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 7.4 Hyperparameter Tuning (optional, for more complex models)\n",
    "\n",
    "Hyperparameter tuning involves searching for the best combination of parameters to improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buLVKO_be7AC"
   },
   "source": [
    "## 9. Submission\n",
    "********************************\n",
    "Ensure your notebook runs end-to-end without errors. Save your notebook and required dependencies in a requirements.txt file. Submit your notebook to Zindi for automated evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-14d6TEe7AC"
   },
   "outputs": [],
   "source": [
    "# Save the model (if required)\n",
    "\n",
    "\n",
    "print(\"Notebook is ready for submission!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
